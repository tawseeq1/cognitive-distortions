{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Cxh-4Ff1NeW",
        "outputId": "e1b54214-70e4-448c-a53e-dd304070a8ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import nltk\n",
        "from nltk import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "import time\n",
        "nltk.download('punkt')\n",
        "\n",
        "data_comments_original1  = pd.read_csv('/content/drive/MyDrive/Cognitive_Distortions_DrFarhanAli_Tawseeq/Dataset/teen_sampled_comments_1.csv')\n",
        "data_comments_original2  = pd.read_csv('/content/drive/MyDrive/Cognitive_Distortions_DrFarhanAli_Tawseeq/Dataset/teen_sampled_comments_2.csv')\n",
        "data_comments_original3  = pd.read_csv('/content/drive/MyDrive/Cognitive_Distortions_DrFarhanAli_Tawseeq/Dataset/teen_sampled_comments_3.csv', on_bad_lines='skip', engine='python')\n",
        "data_comments_original4  = pd.read_csv('/content/drive/MyDrive/Cognitive_Distortions_DrFarhanAli_Tawseeq/Dataset/teen_sampled_comments_4.csv', on_bad_lines='skip', engine='python')\n",
        "\n",
        "data_posts_original  = pd.read_csv('/content/drive/MyDrive/Cognitive_Distortions_DrFarhanAli_Tawseeq/Dataset/teen_sampled_posts.csv')\n",
        "data_comments_original = pd.concat([data_comments_original1, data_comments_original2, data_comments_original3, data_comments_original4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5MuU-QF2Ma-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Cognitive_Distortions_DrFarhanAli_Tawseeq')\n",
        "import targetwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1kMNeXE2M3x"
      },
      "outputs": [],
      "source": [
        "n_orders = [1, 2, 3, 4, 5]\n",
        "def generate_ngrams(tokens, n_orders):\n",
        "    ngram_data = {n: [] for n in n_orders}\n",
        "    for n in n_orders:\n",
        "        ngram_data[n] = [' '.join(ngram) for ngram in ngrams(tokens, n)]\n",
        "    return ngram_data\n",
        "distortion_ngram_sets = {distortion: set(targetwords.__dict__[distortion]) for distortion in targetwords.__dict__.keys() if 'target_' in distortion}\n",
        "\n",
        "def process_data_comments(ngram_data_list, original_data):\n",
        "    results = []\n",
        "    for i, ngram_data in enumerate(ngram_data_list):\n",
        "        distortion_counts = {distortion: 0 for distortion in distortion_ngram_sets.keys()}\n",
        "        for n in n_orders:\n",
        "            for ngram_str in ngram_data[n]:\n",
        "                for distortion in distortion_counts.keys():\n",
        "                    if ngram_str in distortion_ngram_sets[distortion]:\n",
        "                        distortion_counts[distortion] += 1\n",
        "        row = {'Comment No.': original_data.index[i]}\n",
        "        row.update(distortion_counts)\n",
        "        results.append(row)\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def process_data_posts(ngram_data_list_title, ngram_data_list_body, original_data):\n",
        "    results = []\n",
        "    for i in range(len(ngram_data_list_title)):\n",
        "        distortion_counts = {distortion: 0 for distortion in distortion_ngram_sets.keys()}\n",
        "        \n",
        "        for n in n_orders:\n",
        "            for ngram_str in ngram_data_list_title[i][n]:\n",
        "                for distortion in distortion_counts.keys():\n",
        "                    if ngram_str in distortion_ngram_sets[distortion]:\n",
        "                        distortion_counts[distortion] += 1\n",
        "        \n",
        "        for n in n_orders:\n",
        "            for ngram_str in ngram_data_list_body[i][n]:\n",
        "                for distortion in distortion_counts.keys():\n",
        "                    if ngram_str in distortion_ngram_sets[distortion]:\n",
        "                        distortion_counts[distortion] += 1\n",
        "        row = {'Post No.': original_data.index[i]}\n",
        "        row.update(distortion_counts)\n",
        "        results.append(row)\n",
        "    return pd.DataFrame(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LVo0wM62O83"
      },
      "outputs": [],
      "source": [
        "#tokenizing\n",
        "tokenized_data_comments = [word_tokenize(str(text).lower()) for text in data_comments_original['body']]\n",
        "all_ngrams = [generate_ngrams(tokens, n_orders) for tokens in tokenized_data_comments]\n",
        "final_results = process_data_comments(all_ngrams, data_comments_original)\n",
        "final_results.to_csv('/content/drive/MyDrive/Cognitive_Distortions_DrFarhanAli_Tawseeq/Teenagers/EXCEL/teen_all_comments.csv', index=False)\n",
        "\n",
        "tokenized_title = [word_tokenize(str(text).lower()) for text in data_posts_original['title']]\n",
        "tokenized_body = [word_tokenize(str(text).lower()) for text in data_posts_original['body']]\n",
        "all_ngrams_title = [generate_ngrams(tokens, n_orders) for tokens in tokenized_title]\n",
        "all_ngrams_body = [generate_ngrams(tokens, n_orders) for tokens in tokenized_body]\n",
        "final_results_posts = process_data_posts(all_ngrams_title, all_ngrams_body, data_posts_original)\n",
        "\n",
        "final_results_posts.to_csv('your_path', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5PAtfFZJ4Fcu"
      },
      "outputs": [],
      "source": [
        "covid_start = 1579478400  \n",
        "covid_end = 1594598400    \n",
        "end_date = 1697372672\n",
        "\n",
        "before_covid = data_comments_original[data_comments_original['created_utc'] < covid_start]\n",
        "during_covid = data_comments_original[(data_comments_original['created_utc'] >= covid_start) & (data_comments_original['created_utc'] <= covid_end)]\n",
        "after_covid = data_comments_original[(data_comments_original['created_utc'] > covid_end) & (data_comments_original['created_utc'] <= end_date)]\n",
        "\n",
        "tokenized_comments_before_covid_body = [word_tokenize(str(text).lower()) for text in before_covid['body']]\n",
        "tokenized_comments_during_covid_body = [word_tokenize(str(text).lower()) for text in during_covid['body']]\n",
        "tokenized_comments_after_covid_body = [word_tokenize(str(text).lower()) for text in after_covid['body']]\n",
        "\n",
        "all_ngrams_before = [generate_ngrams(tokens, n_orders) for tokens in tokenized_comments_before_covid_body]\n",
        "all_ngrams_during = [generate_ngrams(tokens, n_orders) for tokens in tokenized_comments_during_covid_body]\n",
        "all_ngrams_after = [generate_ngrams(tokens, n_orders) for tokens in tokenized_comments_after_covid_body]\n",
        "\n",
        "final_results_comments_before = process_data_comments(all_ngrams_before, before_covid)\n",
        "final_results_comments_during = process_data_comments(all_ngrams_during, during_covid)\n",
        "final_results_comments_after = process_data_comments(all_ngrams_after, after_covid)\n",
        "\n",
        "final_results_comments_before.to_csv('your_path', index=False)\n",
        "final_results_comments_during.to_csv('your_path', index=False)\n",
        "final_results_comments_after.to_csv('your_path', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hpKG8Is4o1_"
      },
      "outputs": [],
      "source": [
        "covid_start = 1579478400  \n",
        "covid_end = 1594598400    \n",
        "end_date = 1697372672\n",
        "\n",
        "before_covid = data_posts_original[data_posts_original['created_utc'] < covid_start]\n",
        "during_covid = data_posts_original[(data_posts_original['created_utc'] >= covid_start) & (data_posts_original['created_utc'] <= covid_end)]\n",
        "after_covid = data_posts_original[(data_posts_original['created_utc'] > covid_end) & (data_posts_original['created_utc'] <= end_date)]\n",
        "\n",
        "tokenized_posts_before_covid_body = [word_tokenize(str(text).lower()) for text in before_covid['body']]\n",
        "tokenized_posts_during_covid_body = [word_tokenize(str(text).lower()) for text in during_covid['body']]\n",
        "tokenized_posts_after_covid_body = [word_tokenize(str(text).lower()) for text in after_covid['body']]\n",
        "\n",
        "tokenized_posts_before_covid_title = [word_tokenize(str(text).lower()) for text in before_covid['title']]\n",
        "tokenized_posts_during_covid_body = [word_tokenize(str(text).lower()) for text in during_covid['title']]\n",
        "tokenized_posts_after_covid_body = [word_tokenize(str(text).lower()) for text in after_covid['title']]\n",
        "\n",
        "all_ngrams_posts_before_body = [generate_ngrams(tokens, n_orders) for tokens in tokenized_posts_before_covid_body]\n",
        "all_ngrams_posts_during_body = [generate_ngrams(tokens, n_orders) for tokens in tokenized_posts_during_covid_body]\n",
        "all_ngrams_posts_after_body = [generate_ngrams(tokens, n_orders) for tokens in tokenized_posts_after_covid_body]\n",
        "\n",
        "all_ngrams_posts_before_title = [generate_ngrams(tokens, n_orders) for tokens in tokenized_posts_before_covid_title]\n",
        "all_ngrams_posts_during_title = [generate_ngrams(tokens, n_orders) for tokens in tokenized_posts_during_covid_body]\n",
        "all_ngrams_posts_after_title = [generate_ngrams(tokens, n_orders) for tokens in tokenized_posts_after_covid_body]\n",
        "\n",
        "final_results_posts_before = process_data_posts(all_ngrams_posts_before_title, all_ngrams_posts_before_body, before_covid)\n",
        "final_results_posts_during = process_data_posts(all_ngrams_posts_during_title, all_ngrams_posts_during_body, during_covid)\n",
        "final_results_posts_after = process_data_posts(all_ngrams_posts_after_title, all_ngrams_posts_after_body, after_covid)\n",
        "\n",
        "final_results_posts_before.to_csv('your_path', index=False)\n",
        "final_results_posts_during.to_csv('your_path', index=False)\n",
        "final_results_posts_after.to_csv('your_path', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
